{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read .mat data in github\n",
    "\n",
    "link of the code and data: https://github.com/pbashivan/EEGLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for Visual WM experiment described in:\n",
    "Bashivan, Pouya, et al. \"Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks.\" International conference on learning representations (2016).\n",
    "\n",
    "FeatureMat_timeWin:\n",
    "FFT power values extracted for three frequency bands (theta, alpha, beta). Features are arranged in band and electrodes order (theta_1, theta_2..., theta_64, alpha_1, alpha_2, ..., beta_64). There are seven time windows, features for each time window are aggregated sequentially (i.e. 0:191 --> time window 1, 192:383 --> time windw 2 and so on. Last column contains the class labels (load levels).\n",
    "\n",
    "Neuroscan_locs_orig:\n",
    "3 dimensional coordinates for electrodes on Neuroscan quik-cap.\n",
    "\n",
    "trials_subNums:\n",
    "contains subject numbers associated with each trial (used for leave-subject-out cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    " \n",
    "\n",
    "data_dict = scio.loadmat('./data/FeatureMat_timeWin.mat')\n",
    "coordinates_dict = scio.loadmat('./data/Neuroscan_locs_orig.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'A'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()\n",
    "coordinates_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = coordinates_dict['A']\n",
    "df_features = data_dict['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2670, 1345)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.shape\n",
    "# coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.15085451,  1.02301335,  0.48282339, ...,  0.59169703,\n",
       "       -0.01846558,  1.        ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we know that, their data has 64 signal channel, each electrode has 192 feature(alpha beta theta * 64), and each data has 7 time windows, so in total 1344 data + 1 label for one line. and this data has 2670 small set for train;\n",
    "\n",
    "We can also find the head plot function in OpenBCI GUI, is kind of like this process.\n",
    "\n",
    "\n",
    "![head_electrode_position](./data/img_notebook/Cyton_16Ch_Static_Headplot_Image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above paper is not the only way that we could test, and because of data size limitations, we may consider data augmentation methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection with Hugo has been very successful, he did some test for the motion imagination, so for me, I will start from object imagination!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for how to prepare the equipment for data collections and do the experiments:\n",
    "\n",
    "1. Please read https://docs.openbci.com/AddOns/Headwear/GelfreeElectrodeCap/\n",
    "2. For the document in 1, I also made a brief summary after reading it. You can also read my summary to get the environment configuration and other related information. https://xulintaofr.gitbook.io/eeg/\n",
    "\n",
    "(This gitbook was originally designed to organize all the practice content during my internship, but due to time constraints, only the configuration of the device and the functional details of the equipment were summarized.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental procedure\n",
    "\n",
    "0. If the electrodes are not connected, please follow this picture and connect them.\n",
    "\n",
    "![OpenBCI_channels](./data/img_notebook/channels.jpg)\n",
    "\n",
    "1. Get the device and check everything is right;\n",
    "\n",
    "2. Make the saline solution by dissolving 1 tea spoon of sodium chloride (~6 g) into about 200 mL tap water with the cup. Soak the Hydro-linkTM( the white things) sponges in the saline solution for 5-10 min;\n",
    "\n",
    "3. Make sure you don't put the saline solution at another table, do not put them with our board in same place!!!!!!!!!!\n",
    "\n",
    "4. Connect USB to the computer, switch ''OFF'' to ''PC'' in our board, open the OpenBCI GUI application;\n",
    "\n",
    "5. In the application page, chick the \" CYTON(live)\", then click \"Serial(from Dongle)\", click \"Manual\" to choose our port, then choose \"16 CHANNELS\"; You can change the setting - SESSION DATA Name and Max File Duration,then click \"Start session\" and waiting your signals.\n",
    "\n",
    "![OpenBCI_GUI](./data/img_notebook/GUI.jpg)\n",
    "\n",
    "And for Europe we should change to Notch to 50Hz!\n",
    "\n",
    "6. Insert the Hydro-linkTM into the electrode holder of the electrode lead wire.\n",
    "\n",
    "7. Place the cap on the subject's head. Make sure the Cz is correctly located halfway on the midline of the head.\n",
    "\n",
    "8. Do your own experiments, you have better not wear the cap more than 30 minutes/ 1 hour.\n",
    "\n",
    "9. Turn off your GUI application and switch ''PC'' to ''OFF''\n",
    "\n",
    "10. After the measurement is finished, disconnect the electrode connectors from the CytonDaisy board. Remove the electrode holders out of the bases on the cap. 1) Take the Hydro-linkTM sponges out of the holders and wash the Hydro-linkTM with tap water for at least three times, while squeezing out the water in the Hydro-linkTM during washing. Then let the Hydro-linkTM dry in air. 2) Rinse the electrode holders and the Gelfree BCI cap with tap water. 3) Ensure that everything is dry before storing in a dry place away from sunlight\n",
    "\n",
    "11. Cleaning frequency-after each use.\n",
    "\n",
    "12. If you won't use the cap in a long time, please disconnect the electrodes and clean the cap.\n",
    "\n",
    "\n",
    "\n",
    "PS: you can find your recoding txt files in C:\\Users\\XXX(i.e.Lintao XU)\\Documents\\OpenBCI_GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my experiments, I prefer to do the experiments like the OpenBCI and create experiment' videos to help control the timing of the experiment and determine the signal interval.\n",
    "\n",
    "(Files all in the folder my experiment!)\n",
    "\n",
    "Objects I will use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASL\n",
    "* American Sign language Letters\n",
    "1. Vision (observe the letters)\n",
    "2. Imaginations (imagine the letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAT\n",
    "* Alphabet Letters\n",
    "1. Vision (observe the letters)\n",
    "2. Imaginations (imagine the letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big problem that we have is the dataset size! \n",
    "For DL we need data for model training!\n",
    "\n",
    "\n",
    "May consider the public dataset!\n",
    "But remember the initial idea for our project is, create the device for one person, not for all person.\n",
    "\n",
    "(in fact no such data set is open!...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes for creating videos in the rep ''my_experiment'', are the official codes of OpenBCI(rep-OpenBCI_official_experiment), but I modify the codes for our needs.\n",
    "\n",
    "### REP(folders) FILES\n",
    "\n",
    "* alphabet_sounds: One person read all alphabets letters, and modified audio files.\n",
    "\n",
    "* asl_piece: Some pieces videos for asl, make sure delete all files before you run code video_audio_asl.py.\n",
    "\n",
    "* experiments_videos: All experiments' videos for asl and alphabets.\n",
    "\n",
    "* Images: Original images and new named images, and welcome/video beginning images.\n",
    "\n",
    "* labels_alphabets: Labels order for the videos generated by the file video_creator_alphabet.py, and these labels will be used by the file video_audio_asl.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code files\n",
    "\n",
    "* video_images_prepare.py: Randomly select 5(defined) images of each type from the data of the two original image datasets, and copy and rename them to another folder(Letter_class and Sign_class).\n",
    "\n",
    "<code>python video_images_prepare.py</code>\n",
    "\n",
    "* audio_add.py: Code(ffmpeg) to extend audio, merge audio with video, concat videos!\n",
    "\n",
    "* settings.json: Default setting, not modify them unless you have to.\n",
    "\n",
    "* join.txt: File for ffmpeg to merge two videos, automatic modified by codes.\n",
    "\n",
    "* video_creator_alphabet.py: Generate a fixed-length alphabets video, modify the video name and txt name in the main function.\n",
    "\n",
    "<code>python video_creator_alphabet.py -1 settings.json</code>\n",
    "\n",
    "* video_audio_asl.py: Generate a fixed-length asl video by the given alphabet_labels order with audio for each sign letter, modify the video name and given alphabet_labels txt name in the main function.\n",
    "\n",
    "<code>python video_audio_asl.py -1 settings.json</code>\n",
    "\n",
    "* video_creator_asl.py: Generate a fixed-length asl video, modify the video name and txt name in the main function. This file no longer use!!!\n",
    "\n",
    "<code>python video_creator_asl.py -1 settings.json</code>\n",
    "\n",
    "(If you are reading this notebook and have some questions after you have finished my codes reading, please contact me.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the video asl, I took the advice from prof, I added the voice to read ''A, B, C...'', but I really think it's better to remember the ASL instead of add voice to the video, because I think it'll be harder when you want to think the ASL in the imagination part(you could easily directly think of alphabet letters rather than sign letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the video design,\n",
    "\n",
    "for now, in each videos, Images will show 3.5 seconds, and then you need to think/imagine the images 3 seconds.\n",
    "\n",
    "The video is so long, so keep patients when do the experiments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:\n",
    "\n",
    "For some unknown reasons, I missed letter \"X\" in alphabet videos, besides, because the video pieces are generates in \n",
    "\n",
    "2.5 sec black + 3 sec images + 0.5 sec black\n",
    "\n",
    "so the last image of each video will lack time to imagine it.\n",
    "\n",
    "So I do another video and add 30 times letter X and 6 last missed letters(4-Y, 2-G) and other letters A-Z twice. \n",
    "\n",
    "And the videos for asl are kept same with alphabets labels order and videos!\n",
    "\n",
    "(PS: After I found I missed time of imagination for last letter, I fixed this problem in the codes, X videos, so if re run these codes, videos will have time to imagine the last letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in total, I have signal data for ASL and Alphabet:\n",
    "\n",
    "* 6 videos(each one have 125 letters and 124 have both imaginations and visions)\n",
    "* X extensional video(30 X, 2*26 A-Z, 4 Y, 2 G, totally 88 data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other future video design ways.\n",
    "\n",
    "1. Make the each image time longer than 3 seconds etc\n",
    "\n",
    "2. Just voice and no images, closed eyes and imagine our letters.\n",
    "\n",
    "3. For the ASL part, need to re consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
